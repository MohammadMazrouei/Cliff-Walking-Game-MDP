{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nessary libraries\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this class\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "image_path = path.join(path.dirname(gym.__file__), \"envs\", \"toy_text\")\n",
    "\n",
    "class CliffWalking(CliffWalkingEnv):\n",
    "    def __init__(self, is_hardmode=True, num_cliffs=10, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.is_hardmode = is_hardmode\n",
    "\n",
    "        # Generate random cliff positions\n",
    "        if self.is_hardmode:\n",
    "            self.num_cliffs = num_cliffs\n",
    "            self._cliff = np.zeros(self.shape, dtype=bool)\n",
    "            self.start_state = (3, 0)\n",
    "            self.terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
    "            self.cliff_positions = []\n",
    "            while len(self.cliff_positions) < self.num_cliffs:\n",
    "                new_row = np.random.randint(0, 4)\n",
    "                new_col = np.random.randint(0, 11)\n",
    "                state = (new_row, new_col)\n",
    "                if (\n",
    "                    (state not in self.cliff_positions)\n",
    "                    and (state != self.start_state)\n",
    "                    and (state != self.terminal_state)\n",
    "                ):\n",
    "                    self._cliff[new_row, new_col] = True\n",
    "                    if not self.is_valid():\n",
    "                        self._cliff[new_row, new_col] = False\n",
    "                        continue\n",
    "                    self.cliff_positions.append(state)\n",
    "\n",
    "        self.P = {}\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            self.P[s] = {a: [] for a in range(self.nA)}\n",
    "            self.P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n",
    "            self.P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n",
    "            self.P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n",
    "            self.P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n",
    "\n",
    "    def _calculate_transition_prob(self, current, delta):\n",
    "        new_position = np.array(current) + np.array(delta)\n",
    "        new_position = self._limit_coordinates(new_position).astype(int)\n",
    "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "        if self._cliff[tuple(new_position)]:\n",
    "            return [(1.0, self.start_state_index, -100, False)]\n",
    "\n",
    "        terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
    "        is_terminated = tuple(new_position) == terminal_state\n",
    "        return [(1 / 3, new_state, -1, is_terminated)]\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(self):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((3, 0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r, c) in discovered:\n",
    "                discovered.add((r, c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= self.shape[0] or c_new < 0 or c_new >= self.shape[1]:\n",
    "                        continue\n",
    "                    if (r_new, c_new) == self.terminal_state:\n",
    "                        return True\n",
    "                    if not self._cliff[r_new][c_new]:\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    def step(self, action):\n",
    "        if action not in [0, 1, 2, 3]:\n",
    "            raise ValueError(f\"Invalid action {action}   must be in [0, 1, 2, 3]\")\n",
    "\n",
    "        if self.is_hardmode:\n",
    "            match action:\n",
    "                case 0:\n",
    "                    action = np.random.choice([0, 1, 3], p=[1 / 3, 1 / 3, 1 / 3])\n",
    "                case 1:\n",
    "                    action = np.random.choice([0, 1, 2], p=[1 / 3, 1 / 3, 1 / 3])\n",
    "                case 2:\n",
    "                    action = np.random.choice([1, 2, 3], p=[1 / 3, 1 / 3, 1 / 3])\n",
    "                case 3:\n",
    "                    action = np.random.choice([0, 2, 3], p=[1 / 3, 1 / 3, 1 / 3])\n",
    "\n",
    "        return super().step(action)\n",
    "\n",
    "    def _render_gui(self, mode):\n",
    "        try:\n",
    "            import pygame\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gymnasium[toy-text]`\"\n",
    "            ) from e\n",
    "        if self.window_surface is None:\n",
    "            pygame.init()\n",
    "\n",
    "            if mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                pygame.display.set_caption(\"CliffWalking - Edited by Audrina & Kian\")\n",
    "                self.window_surface = pygame.display.set_mode(self.window_size)\n",
    "            else:  # rgb_array\n",
    "                self.window_surface = pygame.Surface(self.window_size)\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        if self.elf_images is None:\n",
    "            hikers = [\n",
    "                path.join(image_path, \"img/elf_up.png\"),\n",
    "                path.join(image_path, \"img/elf_right.png\"),\n",
    "                path.join(image_path, \"img/elf_down.png\"),\n",
    "                path.join(image_path, \"img/elf_left.png\"),\n",
    "            ]\n",
    "            self.elf_images = [\n",
    "                pygame.transform.scale(pygame.image.load(f_name), self.cell_size)\n",
    "                for f_name in hikers\n",
    "            ]\n",
    "        if self.start_img is None:\n",
    "            file_name = path.join(image_path, \"img/stool.png\")\n",
    "            self.start_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.goal_img is None:\n",
    "            file_name = path.join(image_path, \"img/cookie.png\")\n",
    "            self.goal_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.mountain_bg_img is None:\n",
    "            bg_imgs = [\n",
    "                path.join(image_path, \"img/mountain_bg1.png\"),\n",
    "                path.join(image_path, \"img/mountain_bg2.png\"),\n",
    "            ]\n",
    "            self.mountain_bg_img = [\n",
    "                pygame.transform.scale(pygame.image.load(f_name), self.cell_size)\n",
    "                for f_name in bg_imgs\n",
    "            ]\n",
    "        if self.near_cliff_img is None:\n",
    "            near_cliff_imgs = [\n",
    "                path.join(image_path, \"img/mountain_near-cliff1.png\"),\n",
    "                path.join(image_path, \"img/mountain_near-cliff2.png\"),\n",
    "            ]\n",
    "            self.near_cliff_img = [\n",
    "                pygame.transform.scale(pygame.image.load(f_name), self.cell_size)\n",
    "                for f_name in near_cliff_imgs\n",
    "            ]\n",
    "        if self.cliff_img is None:\n",
    "            file_name = path.join(image_path, \"img/mountain_cliff.png\")\n",
    "            self.cliff_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            row, col = np.unravel_index(s, self.shape)\n",
    "            pos = (col * self.cell_size[0], row * self.cell_size[1])\n",
    "            check_board_mask = row % 2 ^ col % 2\n",
    "            self.window_surface.blit(self.mountain_bg_img[check_board_mask], pos)\n",
    "\n",
    "            if self._cliff[row, col]:\n",
    "                self.window_surface.blit(self.cliff_img, pos)\n",
    "            if s == self.start_state_index:\n",
    "                self.window_surface.blit(self.start_img, pos)\n",
    "            if s == self.nS - 1:\n",
    "                self.window_surface.blit(self.goal_img, pos)\n",
    "            if s == self.s:\n",
    "                elf_pos = (pos[0], pos[1] - 0.1 * self.cell_size[1])\n",
    "                last_action = self.lastaction if self.lastaction is not None else 2\n",
    "                self.window_surface.blit(self.elf_images[last_action], elf_pos)\n",
    "\n",
    "        if mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.window_surface)), axes=(1, 0, 2)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "env = CliffWalking(render_mode=\"human\")\n",
    "observation, info = env.reset(seed=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Making Process\n",
    "### What is the problem\n",
    "The problem in general consist of a four tuple $(S, A, P, R)$ in which:\n",
    "\n",
    "$S$: set of states _(state space).\n",
    "\n",
    "$A(s)$: represents set of actions available in state $s \\in S$.\n",
    "\n",
    "$R(s, a, s')$: immidiate reward of going from $s$ to $s'$ with by action $a$.\n",
    "\n",
    "$P(s, a, s')$: probability of going to $s'$ when applying action $a$ in state $s$.\n",
    "\n",
    "Agent starts at an initial state $s_0$ and chooses some action $a_0$ and it will move to a new state $s'$ with probability $P(s_0, a_0, s')$ and immidiate reward $R(s_0, a_0, s')$. and this process continues untill the agent reaches a goal or a dead state, forcing it to halt.\n",
    "\n",
    "What we are looking for is a policy $\\pi : S \\rightarrow A$ which tells the agent what action to choose when being in an arbitrary state, so that it maximizes the expected total reward from playing once.\n",
    "\n",
    "### Solution and Algorithm\n",
    "In order to solve problem we introduce two notations.$ V: S \\rightarrow \\mathbb{R} $, which assigns expected reward value when we're in some state to that state, and $Q: S \\times A \\rightarrow \\mathbb{R}$, which shows the expected reward value of perfoming some action in some state.\n",
    "\n",
    "Let's say we sart with a random policy $\\pi$ and we want to find $\\pi^*$ which is the best policy.\n",
    "\n",
    "This asgorithm iterates over and over until convergence in policy or we reach maximum number of iterations. In each iteration we do two things,  update $Q$, update $V$. here's how we do that:\n",
    "\n",
    "-----------------\n",
    "\n",
    "#### update $Q$:\n",
    "We use preveous values of $V$ to update $Q$, with this expression:\n",
    "\n",
    "$$ Q(s, a) = \\sum_{s' \\in S} P(s, a, s')(R(s, a, s')+ \\gamma V(s')) $$\n",
    "\n",
    "where $0 \\lt \\gamma \\lt 1$ is the discount factor.\n",
    "\n",
    "------------------\n",
    "\n",
    "#### update $V$:\n",
    "Using updated values of $Q$ we can update $V$ as follows:\n",
    "\n",
    "$$ V(s) = max_{a \\in A(s)} Q(s, a) $$\n",
    "\n",
    "------------------\n",
    "\n",
    "At the end we use $Q$ to find the policy this way:\n",
    "$$\\pi(s) = argmax_{a \\in A(s)} Q(s, a) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "#### Class MDP\n",
    "\n",
    "The MDP class represents a Markov Decision Process (MDP), which is a mathematical framework for modeling decision-making under uncertainty. An MDP consists of a set of states, a set of actions, a transition function, a reward function, and a discount factor.\n",
    "\n",
    "In this code, the __init__() method initializes the MDP object. It takes three parameters:\n",
    "max_iterations: The maximum number of iterations to run the policy iteration algorithm.\n",
    "discount_factor: The discount factor, which determines how much the agent values future rewards compared to immediate rewards.\n",
    "cliff_positions: A list of positions that represent the cliffs.\n",
    "The V array stores the value of each state, the Q array stores the value of taking each action in each state, and the pi array stores the policy, which is a mapping from each state to an action.\n",
    "\n",
    "The update_QV() method then updates the Q-function (Q) by adding the expected reward to the current Q-value for that state and action. This ensures that the Q-value for that state and action reflects the expected long-term reward.\n",
    "The update_QV() method then updates the value function (V) by finding the maximum Q-value for that state. This ensures that the value function reflects the maximum expected long-term reward from any state.\n",
    "The update_QV() method is called iteratively until the value function converges. This means that the value function no longer changes significantly between iterations.\n",
    "\n",
    "The update_pi() method updates the policy (pi) by iterating over each state (i, j) and action (k). It then determines the action that has the highest Q-value for that state and sets the policy for that state to that action. This ensures that the policy reflects the action that will maximize the expected long-term reward for that state.\n",
    "\n",
    "The run() method runs the policy iteration algorithm. It initially sets the value of the goal state to a high value and the value of the cliff states to a low value. It then repeatedly updates the value function (V) and the policy (pi) until it converges.\n",
    "\n",
    "The policy() method returns the action that should be taken in a given state. It does this by retrieving the corresponding action from the policy (pi) for that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, max_iterations, discount_factor, cliff_positions):\n",
    "        self.V = np.zeros((4, 12))\n",
    "        self.Q = np.zeros((4, 12, 4))\n",
    "        self.pi = np.zeros((4, 12), dtype=int)\n",
    "        self.max_iter = max_iterations\n",
    "        self.gama = discount_factor\n",
    "        self.cliffs = cliff_positions\n",
    "        self.max_converged = 100\n",
    "\n",
    "        self.action_reward = -1\n",
    "        self.goal_reward = 4000\n",
    "        self.cliff_reward = -100\n",
    "\n",
    "    \n",
    "    def update_QV(self):\n",
    "        di = [-1, 0, 1, 0]\n",
    "        dj = [0, 1, 0, -1]\n",
    "        # update Q\n",
    "        for i in range(4):\n",
    "            for j in range(12):\n",
    "                for k in range(4):\n",
    "                    \n",
    "                    num = 0\n",
    "                    for action in [UP, RIGHT, DOWN, LEFT]:\n",
    "                        if k != action and action%2 == k%2:\n",
    "                            continue\n",
    "                        next_state = (i+di[action], j+dj[action])\n",
    "                        if next_state[0] >= 4 or next_state[1] >= 12 or next_state[0] < 0 or next_state[1] < 0:\n",
    "                            num += 1\n",
    "                    prob = 1/3\n",
    "                    if (num == 2):\n",
    "                        prob = 1\n",
    "                    elif (num == 1):\n",
    "                        prob = 1/2\n",
    "                    \n",
    "                    self.Q[i, j, k] = 0\n",
    "                    for action in [UP, RIGHT, DOWN, LEFT]:\n",
    "                        if k != action and action%2 == k%2:\n",
    "                            continue\n",
    "                        next_state = (i+di[action], j+dj[action])\n",
    "                        if next_state[0] >= 4 or next_state[1] >= 12 or next_state[0] < 0 or next_state[1] < 0:\n",
    "                            continue\n",
    "                        self.Q[i, j, k] += prob * (self.action_reward + self.gama * self.V[next_state[0], next_state[1]])\n",
    "\n",
    "        # update V\n",
    "        for i in range(4):\n",
    "            for j in range(12):\n",
    "                if i == 3 and j == 11:\n",
    "                    continue\n",
    "                if (i, j) in self.cliffs:\n",
    "                    continue\n",
    "                self.V[i, j] = max(self.Q[i, j])\n",
    "                \n",
    "                    \n",
    "    def update_pi(self):\n",
    "        for i in range(4):\n",
    "            for j in range(12):\n",
    "                for action in [UP, RIGHT, DOWN, LEFT]:\n",
    "                    if self.Q[i, j, action] == self.V[i, j]:\n",
    "                        self.pi[i, j] = action\n",
    "                        \n",
    "\n",
    "    def run(self):\n",
    "        self.V[3, 11] = self.goal_reward\n",
    "        for ci, cj in self.cliffs:\n",
    "            self.V[ci, cj] = self.cliff_reward\n",
    "            \n",
    "        iter_cnt = 0\n",
    "        converged = 0\n",
    "        while iter_cnt < self.max_iter or converged < self.max_converged:\n",
    "            tmp_V = np.array(self.V, copy = True)\n",
    "            self.update_QV()\n",
    "            if self.V.all() == tmp_V.all():\n",
    "                converged += 1\n",
    "            else:\n",
    "                converged = 0\n",
    "            iter_cnt += 1\n",
    "\n",
    "        self.update_pi()\n",
    "\n",
    "    \n",
    "    def policy(self, state):\n",
    "        return self.pi[state[0], state[1]]\n",
    "\n",
    "    \n",
    "    def print_pi(self):\n",
    "        direction = {0: '^', 1: '>', 2: 'V', 3: '<'}\n",
    "        for i in range(4):\n",
    "            for j in range(12):\n",
    "                print(direction[self.\n",
    "                      pi[i, j]], end=\" \")\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "    \n",
    "    def print_v(self):\n",
    "        for i in range(4):\n",
    "            for j in range(12):\n",
    "                print(round(self.V[i, j], 2), end=\"\\t\")\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< > > < ^ > ^ < ^ > > > \n",
      "< > > < ^ < ^ > V > > > \n",
      "^ > > ^ V ^ V > > > ^ > \n",
      "V > < ^ < ^ > > > < ^ ^ \n",
      "\n",
      "-100.0\t472.24\t482.23\t486.68\t-100.0\t1185.25\t1644.22\t2138.42\t-100.0\t3465.45\t3572.7\t3654.11\t\n",
      "-100.0\t473.8\t489.54\t502.99\t-100.0\t752.25\t-100.0\t2677.84\t3058.75\t3430.23\t3565.48\t3692.03\t\n",
      "-100.0\t477.02\t501.27\t551.01\t668.48\t1197.33\t2210.57\t2920.53\t3163.9\t3366.75\t3542.8\t3806.55\t\n",
      "467.73\t473.46\t481.5\t-100.0\t280.4\t-100.0\t2583.87\t3011.39\t3165.1\t3232.27\t-100.0\t4000.0\t\n",
      "\n",
      "Wins = 1\n"
     ]
    }
   ],
   "source": [
    "# Define the maximum number of iterations\n",
    "max_iter_number = 1000\n",
    "iter_of_mdp = 2000\n",
    "gama = 0.99\n",
    "current_state = (3, 0)\n",
    "wins = 0\n",
    "\n",
    "mdp = MDP(iter_of_mdp, gama, env.cliff_positions)\n",
    "mdp.run()\n",
    "mdp.print_pi()\n",
    "mdp.print_v()\n",
    "\n",
    "for __ in range(max_iter_number):\n",
    "    try:\n",
    "        # Choose an action\n",
    "        action = mdp.policy(current_state)\n",
    "    \n",
    "        # Perform the action and receive feedback from the environment\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        current_state = (next_state//12, next_state%12)\n",
    "    \n",
    "        if done or truncated:\n",
    "            observatistopon, info = env.reset()\n",
    "            current_state = (3, 0)\n",
    "        wins += done\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "print(\"Wins =\", wins)\n",
    "env.close()\n",
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
